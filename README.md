# When XGBoost Outperforms GPT-4 on Text Classification: A Case Study

<p style="color:#808080;"><i>published at <a href="https://trustnlpworkshop.github.io">NAACL 2024 Workshop on Trustworthy Natural Language Processing (TrustNLP 2024)</a></i></p>

[See paper](https://aclanthology.org/2024.trustnlp-1.5/) — [See poster](https://aclanthology.org/2024.trustnlp-1.5/) — [Contact us](mailto:maty-at-stanford-dot-edu)

Large language models (LLMs) are increasingly used for applications beyond text generation, ranging from text summarization to instruction following. One popular example of exploiting LLMs’ zero- and few-shot capabilities is the task of text classification. This short paper compares two popular LLM-based classification pipelines (GPT-4 and LLAMA 2) to a popular pre-LLM-era classification pipeline on the task of news trustworthiness classification, focusing on performance, training, and deployment requirements. We find that, in this case, the pre-LLM-era ensemble pipeline outperforms the two popular LLM pipelines while being orders of magnitude smaller in parameter size.

## Getting Started

TODO

## Reproducing Paper Results

TODO

## Using the Methodology for Custom Projects

TODO

## Citation

```bibtex
@inproceedings{bohacek-bravansky-2024-xgboost,
    title = "When {XGB}oost Outperforms {GPT}-4 on Text Classification: A Case Study",
    author = "Bohacek, Matyas  and
      Bravansky, Michal",
    booktitle = "Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.trustnlp-1.5",
    pages = "51--60"
}
```
